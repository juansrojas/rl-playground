{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1753b0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from src.rl_agent import RLAgent\n",
    "from src.state_representation import StateRepresentation\n",
    "\n",
    "from src.env_bandit import EnvironmentBandit\n",
    "from src.env_parkingworld import ParkingWorld\n",
    "from src.env_cliffwalk import EnvironmentCliffWalk\n",
    "from src.env_maze import EnvironmentMaze\n",
    "from src.env_randomwalk import EnvironmentRandomWalk\n",
    "from src.env_mountaincar import EnvironmentMountainCar\n",
    "from src.env_pendulum import EnvironmentPendulum\n",
    "from src.env_lunarlanding import EnvironmentLunarLanding\n",
    "\n",
    "from src.rl_experiments import RLExperiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ab0f3e",
   "metadata": {},
   "source": [
    "## Bandits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ef1b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define environment\n",
    "n_arms = 10\n",
    "env = EnvironmentBandit(n_arms=n_arms) \n",
    "\n",
    "# define agent\n",
    "actions = list(np.arange(1, n_arms + 1))\n",
    "states = None\n",
    "policy = None\n",
    "agent = RLAgent(agent_type='bandit', \n",
    "                states=states, \n",
    "                actions=actions, \n",
    "                policy=policy, \n",
    "                policy_type='tabular',\n",
    "                value_type='tabular')\n",
    "\n",
    "# run experiment\n",
    "rl_experiments = RLExperiments()\n",
    "df, fig = rl_experiments.bandit(agent, env,\n",
    "                                num_episodes=200,\n",
    "                                num_steps=1000,\n",
    "                                step_size={'value': None},\n",
    "                                epsilon=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4258bc5",
   "metadata": {},
   "source": [
    "## Dynamic Programming - Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8acd15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define environment\n",
    "num_spaces = 10\n",
    "num_prices = 4\n",
    "env = ParkingWorld(num_spaces, num_prices) \n",
    "\n",
    "# define agent\n",
    "actions = env.actions\n",
    "\n",
    "states = env.states\n",
    "\n",
    "policy = {}\n",
    "for state in states:\n",
    "    policy[state] = {}\n",
    "    counter = 0\n",
    "    for a in actions:\n",
    "        if counter == 1:\n",
    "            policy[state][a] = 1\n",
    "        else:\n",
    "            policy[state][a] = 0\n",
    "            \n",
    "        counter += 1 \n",
    "        \n",
    "agent = RLAgent(agent_type='dp', \n",
    "                states=states, \n",
    "                actions=actions, \n",
    "                policy=policy, \n",
    "                policy_type='tabular',\n",
    "                value_type='tabular')\n",
    "\n",
    "# run experiment\n",
    "rl_experiments = RLExperiments()\n",
    "fig = rl_experiments.policy_evaluation(agent, env,\n",
    "                                       discount=0.9, \n",
    "                                       theta=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7cc566",
   "metadata": {},
   "source": [
    "## Dynamic Programming - Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84f9a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define environment\n",
    "num_spaces = 10\n",
    "num_prices = 4\n",
    "env = ParkingWorld(num_spaces, num_prices) \n",
    "\n",
    "# define agent\n",
    "actions = env.actions\n",
    "\n",
    "states = env.states\n",
    "\n",
    "policy = {}\n",
    "for state in states:\n",
    "    policy[state] = {}\n",
    "    for a in actions:\n",
    "        policy[state][a] = 1 / len(actions) \n",
    "        \n",
    "agent = RLAgent(agent_type='dp', \n",
    "                states=states, \n",
    "                actions=actions, \n",
    "                policy=policy, \n",
    "                policy_type='tabular',\n",
    "                value_type='tabular')\n",
    "\n",
    "# run experiment\n",
    "rl_experiments = RLExperiments()\n",
    "fig = rl_experiments.policy_iteration(agent, env,\n",
    "                                      discount=0.9, \n",
    "                                      theta=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00f2bf2",
   "metadata": {},
   "source": [
    "## Dynamic Programming - Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0949c2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define environment\n",
    "num_spaces = 10\n",
    "num_prices = 4\n",
    "env = ParkingWorld(num_spaces, num_prices) \n",
    "\n",
    "# define agent\n",
    "actions = env.actions\n",
    "\n",
    "states = env.states\n",
    "\n",
    "policy = {}\n",
    "for state in states:\n",
    "    policy[state] = {}\n",
    "    for a in actions:\n",
    "        policy[state][a] = 1 / len(actions) \n",
    "        \n",
    "agent = RLAgent(agent_type='dp', \n",
    "                states=states, \n",
    "                actions=actions, \n",
    "                policy=policy, \n",
    "                policy_type='tabular',\n",
    "                value_type='tabular')\n",
    "\n",
    "# run experiment\n",
    "rl_experiments = RLExperiments()\n",
    "fig = rl_experiments.value_iteration(agent, env,\n",
    "                                     discount=0.9, \n",
    "                                     theta=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9602345",
   "metadata": {},
   "source": [
    "## TD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f01202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define environment\n",
    "grid_height = 4\n",
    "grid_width = 12\n",
    "env = EnvironmentCliffWalk(grid_height, grid_width) \n",
    "\n",
    "# define agent\n",
    "actions = ['up', 'left', 'down', 'right']\n",
    "\n",
    "states = []\n",
    "\n",
    "policy = {}\n",
    "for h in range(grid_height):\n",
    "    for w in range(grid_width):\n",
    "        location = (w, h)\n",
    "        state = env.get_state(location)\n",
    "        policy[state] = {}\n",
    "        for a in actions:\n",
    "            policy[state][a] = 1 / len(actions)\n",
    "        states.append(state)        \n",
    "policy[36] = {'up': 0.9, 'left': 0.1/3, 'down': 0.1/3, 'right': 0.1/3}\n",
    "for i in range(24, 35):\n",
    "    policy[i] = {'up': 0.1/3, 'left': 0.1/3, 'down': 0.1/3, 'right': 0.9}\n",
    "policy[35] = {'up': 0.1/3, 'left': 0.1/3, 'down': 0.9, 'right': 0.1/3}\n",
    "        \n",
    "agent = RLAgent(agent_type='td', \n",
    "                states=states, \n",
    "                actions=actions, \n",
    "                policy=policy, \n",
    "                policy_type='tabular',\n",
    "                value_type='tabular',\n",
    "                use_value_trace=False, \n",
    "                value_trace_lambda=0)\n",
    "\n",
    "# run experiment\n",
    "rl_experiments = RLExperiments()\n",
    "df, fig = rl_experiments.td_lambda(agent, env,\n",
    "                                   num_episodes=10000,\n",
    "                                   step_size={'value': 0.1},\n",
    "                                   discount=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999214a1",
   "metadata": {},
   "source": [
    "## SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648a3cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define environment\n",
    "grid_height = 4\n",
    "grid_width = 12\n",
    "env = EnvironmentCliffWalk(grid_height, grid_width) \n",
    "\n",
    "# define agent\n",
    "actions = ['up', 'left', 'down', 'right']\n",
    "\n",
    "states = []\n",
    "for h in range(grid_height):\n",
    "    for w in range(grid_width):\n",
    "        location = (w, h)\n",
    "        state = env.get_state(location)\n",
    "        states.append(state)        \n",
    "\n",
    "policy = None\n",
    "        \n",
    "agent = RLAgent(agent_type='sarsa', \n",
    "                states=states, \n",
    "                actions=actions, \n",
    "                policy=policy, \n",
    "                policy_type='tabular',\n",
    "                value_type='tabular',\n",
    "                use_value_trace=False, \n",
    "                value_trace_lambda=0)\n",
    "\n",
    "# run experiment\n",
    "rl_experiments = RLExperiments()\n",
    "df, fig = rl_experiments.q_learning_sarsa(agent, env,\n",
    "                                          num_runs=100,\n",
    "                                          num_episodes=1000,\n",
    "                                          epsilon=0.1,\n",
    "                                          step_size={'value': 0.5},\n",
    "                                          discount=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfe6fc1",
   "metadata": {},
   "source": [
    "## Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e943714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define environment\n",
    "grid_height = 4\n",
    "grid_width = 12\n",
    "env = EnvironmentCliffWalk(grid_height, grid_width) \n",
    "\n",
    "# define agent\n",
    "actions = ['up', 'left', 'down', 'right']\n",
    "\n",
    "states = []\n",
    "for h in range(grid_height):\n",
    "    for w in range(grid_width):\n",
    "        location = (w, h)\n",
    "        state = env.get_state(location)\n",
    "        states.append(state)        \n",
    "\n",
    "policy = None\n",
    "\n",
    "agent = RLAgent(agent_type='q_learning', \n",
    "                states=states, \n",
    "                actions=actions, \n",
    "                policy=policy, \n",
    "                policy_type='tabular',\n",
    "                value_type='tabular',\n",
    "                use_value_trace=False, \n",
    "                value_trace_lambda=0)\n",
    "\n",
    "# run experiment\n",
    "rl_experiments = RLExperiments()\n",
    "df, fig = rl_experiments.q_learning_sarsa(agent, env,\n",
    "                                          num_runs=100,\n",
    "                                          num_episodes=1000,\n",
    "                                          epsilon=0.1,\n",
    "                                          step_size={'value': 0.5},\n",
    "                                          discount=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788b6202",
   "metadata": {},
   "source": [
    "## Expected SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a288f687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define environment\n",
    "grid_height = 4\n",
    "grid_width = 12\n",
    "env = EnvironmentCliffWalk(grid_height, grid_width) \n",
    "\n",
    "# define agent\n",
    "actions = ['up', 'left', 'down', 'right']\n",
    "\n",
    "states = []\n",
    "for h in range(grid_height):\n",
    "    for w in range(grid_width):\n",
    "        location = (w, h)\n",
    "        state = env.get_state(location)\n",
    "        states.append(state)        \n",
    "\n",
    "policy = None\n",
    "        \n",
    "agent = RLAgent(agent_type='expected_sarsa', \n",
    "                states=states, \n",
    "                actions=actions, \n",
    "                policy=policy, \n",
    "                policy_type='tabular',\n",
    "                value_type='tabular')\n",
    "\n",
    "# run experiment\n",
    "rl_experiments = RLExperiments()\n",
    "df, fig = rl_experiments.q_learning_sarsa(agent, env,\n",
    "                                          num_runs=100,\n",
    "                                          num_episodes=1000,\n",
    "                                          epsilon=0.1,\n",
    "                                          step_size={'value': 0.5},\n",
    "                                          discount=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbce55e4",
   "metadata": {},
   "source": [
    "## Planning - No Planning vs. Planning Number of Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15127d7c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# define environment\n",
    "grid_height = 6\n",
    "grid_width = 9\n",
    "env = EnvironmentMaze(grid_height, grid_width) \n",
    "\n",
    "# define agent\n",
    "actions = ['up', 'left', 'down', 'right']\n",
    "\n",
    "states = []\n",
    "for h in range(grid_height):\n",
    "    for w in range(grid_width):\n",
    "        location = (w, h)\n",
    "        state = env.get_state(location)\n",
    "        states.append(state)        \n",
    "\n",
    "policy = None\n",
    "        \n",
    "agent = RLAgent(agent_type='q_learning', \n",
    "                states=states, \n",
    "                actions=actions, \n",
    "                policy=policy, \n",
    "                policy_type='tabular',\n",
    "                value_type='tabular',\n",
    "                planning_type='dyna',\n",
    "                use_value_trace=False, \n",
    "                value_trace_lambda=0)\n",
    "\n",
    "# run experiment\n",
    "rl_experiments = RLExperiments()\n",
    "results, fig = rl_experiments.dyna_q_planning_num_steps(agent, env,\n",
    "                                                        num_runs=30,\n",
    "                                                        num_episodes=40,\n",
    "                                                        epsilon=0.1,\n",
    "                                                        step_size={'value': 0.125},\n",
    "                                                        discount=0.95,\n",
    "                                                        planning_steps_list=[0, 5, 50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc80805",
   "metadata": {},
   "source": [
    "## Planning - Dyna-Q Planning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac62e8f2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# define environment\n",
    "grid_height = 6\n",
    "grid_width = 9\n",
    "env = EnvironmentMaze(grid_height, grid_width) \n",
    "\n",
    "# define agent\n",
    "actions = ['up', 'left', 'down', 'right']\n",
    "\n",
    "states = []\n",
    "for h in range(grid_height):\n",
    "    for w in range(grid_width):\n",
    "        location = (w, h)\n",
    "        state = env.get_state(location)\n",
    "        states.append(state)        \n",
    "\n",
    "policy = None\n",
    "\n",
    "agent = RLAgent(agent_type='q_learning', \n",
    "                states=states, \n",
    "                actions=actions, \n",
    "                policy=policy, \n",
    "                policy_type='tabular',\n",
    "                value_type='tabular',\n",
    "                planning_type='dyna',\n",
    "                use_value_trace=False, \n",
    "                value_trace_lambda=0)\n",
    "\n",
    "# run experiment\n",
    "rl_experiments = RLExperiments()\n",
    "results, fig = rl_experiments.dyna_q_planning_state_visits(agent, env,\n",
    "                                                           num_runs=5,\n",
    "                                                           num_episodes=500,\n",
    "                                                           epsilon=0.1,\n",
    "                                                           step_size={'value': 0.125},\n",
    "                                                           discount=0.95,\n",
    "                                                           planning_steps=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc5c697",
   "metadata": {},
   "source": [
    "## Planning - Dyna-Q vs. Dyna-Q+ in Changing Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8e3c48",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# define environment\n",
    "grid_height = 6\n",
    "grid_width = 9\n",
    "env = EnvironmentMaze(grid_height, grid_width, obstacle_switch_time=1000) \n",
    "\n",
    "# define agent\n",
    "actions = ['up', 'left', 'down', 'right']\n",
    "\n",
    "states = []\n",
    "for h in range(grid_height):\n",
    "    for w in range(grid_width):\n",
    "        location = (w, h)\n",
    "        state = env.get_state(location)\n",
    "        states.append(state)        \n",
    "\n",
    "policy = None\n",
    "        \n",
    "agent = RLAgent(agent_type='q_learning', \n",
    "                states=states, \n",
    "                actions=actions, \n",
    "                policy=policy, \n",
    "                policy_type='tabular',\n",
    "                value_type='tabular',\n",
    "                planning_type='dyna',\n",
    "                use_value_trace=False, \n",
    "                value_trace_lambda=0)\n",
    "\n",
    "# run experiment\n",
    "rl_experiments = RLExperiments()\n",
    "df, fig = rl_experiments.dyna_q_planning_state_visits(agent, env,\n",
    "                                                      num_runs=5,\n",
    "                                                      num_episodes=500,\n",
    "                                                      epsilon=0.1,\n",
    "                                                      step_size={'value': 0.125},\n",
    "                                                      discount=0.95,\n",
    "                                                      planning_steps=30,\n",
    "                                                      obstacle_switch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b381fe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# define environment\n",
    "grid_height = 6\n",
    "grid_width = 9\n",
    "env = EnvironmentMaze(grid_height, grid_width, obstacle_switch_time=1000) \n",
    "\n",
    "# define agent\n",
    "actions = ['up', 'left', 'down', 'right']\n",
    "\n",
    "states = []\n",
    "for h in range(grid_height):\n",
    "    for w in range(grid_width):\n",
    "        location = (w, h)\n",
    "        state = env.get_state(location)\n",
    "        states.append(state)        \n",
    "\n",
    "policy = None\n",
    "        \n",
    "agent = RLAgent(agent_type='q_learning', \n",
    "                states=states, \n",
    "                actions=actions, \n",
    "                policy=policy, \n",
    "                policy_type='tabular',\n",
    "                value_type='tabular',\n",
    "                planning_type='dyna_plus',\n",
    "                planning_kappa=0.01,\n",
    "                use_value_trace=False, \n",
    "                value_trace_lambda=0)\n",
    "\n",
    "# run experiment\n",
    "rl_experiments = RLExperiments()\n",
    "df, fig = rl_experiments.dyna_q_planning_state_visits(agent, env,\n",
    "                                                      num_runs=5,\n",
    "                                                      num_episodes=500,\n",
    "                                                      epsilon=0.1,\n",
    "                                                      step_size={'value': 0.125},\n",
    "                                                      discount=0.95,\n",
    "                                                      planning_steps=30,\n",
    "                                                      obstacle_switch=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52624124",
   "metadata": {},
   "source": [
    "## Semi-Gradient TD with State Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6caeeef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define environment\n",
    "num_states = 500\n",
    "start_state = 250\n",
    "left_terminal_state = 1\n",
    "right_terminal_state = 500\n",
    "max_movement = 100\n",
    "env = EnvironmentRandomWalk(num_states, start_state, left_terminal_state, right_terminal_state, max_movement) \n",
    "\n",
    "# define agent with function approximation\n",
    "num_groups = 10\n",
    "num_states_in_group = int(num_states / num_groups)\n",
    "state_representation = StateRepresentation(num_states_in_group=num_states_in_group,\n",
    "                                           num_groups=num_groups)\n",
    "\n",
    "actions = ['left', 'right']\n",
    "states = np.zeros((num_groups, 1))\n",
    "policy = None\n",
    "\n",
    "agent = RLAgent(agent_type='td', \n",
    "                states=states, \n",
    "                actions=actions, \n",
    "                policy=policy, \n",
    "                policy_type='tabular',\n",
    "                value_type='linear',\n",
    "                value_update_type='stochastic_gradient_descent',\n",
    "                use_value_trace=False, \n",
    "                value_trace_lambda=0)\n",
    "\n",
    "# run experiment\n",
    "rl_experiments = RLExperiments()\n",
    "fig = rl_experiments.td_semigradient(agent, env, \n",
    "                                     state_representation=state_representation.state_aggregation,\n",
    "                                     num_runs=10,\n",
    "                                     num_episodes=2000,\n",
    "                                     epsilon = 1.0,\n",
    "                                     step_size={'value': 0.05},\n",
    "                                     discount=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7828a4fa",
   "metadata": {},
   "source": [
    "## Semi-Gradient TD with Neural Network (Stochastic Gradient Descent Update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd9d6c1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# define environment\n",
    "num_states = 50\n",
    "start_state = 25\n",
    "left_terminal_state = 1\n",
    "right_terminal_state= 50\n",
    "max_movement = 10\n",
    "env = EnvironmentRandomWalk(num_states, start_state, left_terminal_state, right_terminal_state, max_movement) \n",
    "\n",
    "# define agent with function approximation\n",
    "state_representation = StateRepresentation(num_states=num_states)\n",
    "\n",
    "actions = ['left', 'right']\n",
    "states = np.zeros((num_states, 1))\n",
    "policy = None\n",
    "\n",
    "value_nn_config = {\n",
    "    'layers': {\n",
    "        'input': {\n",
    "            'nodes': len(states),\n",
    "            'activation_function': 'linear',\n",
    "        },\n",
    "        'hidden_1': {\n",
    "            'nodes': 10,\n",
    "            'activation_function': 'relu',\n",
    "        },\n",
    "        'output': {\n",
    "            'nodes': 1,\n",
    "            'activation_function': 'linear',\n",
    "        },\n",
    "    }\n",
    "}\n",
    "            \n",
    "agent = RLAgent(agent_type='td', \n",
    "                states=states, \n",
    "                actions=actions, \n",
    "                policy=policy, \n",
    "                policy_type='tabular',\n",
    "                value_type='neural_network', \n",
    "                value_nn_config=value_nn_config,\n",
    "                value_update_type='stochastic_gradient_descent',\n",
    "                use_value_trace=False, \n",
    "                value_trace_lambda=0)\n",
    "\n",
    "# run experiment\n",
    "rl_experiments = RLExperiments()\n",
    "fig = rl_experiments.td_semigradient(agent, env, \n",
    "                                     state_representation=state_representation.one_hot_encode,\n",
    "                                     num_runs=10,\n",
    "                                     num_episodes=2000,\n",
    "                                     epsilon=1.0,\n",
    "                                     step_size={'value': 0.02},\n",
    "                                     discount=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b798f6",
   "metadata": {},
   "source": [
    "## Semi-Gradient TD with Neural Network (Adam Update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653a6d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define environment\n",
    "num_states = 50\n",
    "start_state = 25\n",
    "left_terminal_state = 1\n",
    "right_terminal_state= 50\n",
    "max_movement = 10\n",
    "env = EnvironmentRandomWalk(num_states, start_state, left_terminal_state, right_terminal_state, max_movement) \n",
    "\n",
    "# define agent with function approximation\n",
    "state_representation = StateRepresentation(num_states=num_states)\n",
    "\n",
    "actions = ['left', 'right']\n",
    "states = np.zeros((num_states, 1))\n",
    "policy = None\n",
    "\n",
    "value_nn_config = {\n",
    "    'layers': {\n",
    "        'input': {\n",
    "            'nodes': len(states),\n",
    "            'activation_function': 'linear',\n",
    "        },\n",
    "        'hidden_1': {\n",
    "            'nodes': 10,\n",
    "            'activation_function': 'relu',\n",
    "        },\n",
    "        'output': {\n",
    "            'nodes': 1,\n",
    "            'activation_function': 'linear',\n",
    "        },\n",
    "    }\n",
    "}\n",
    "            \n",
    "agent = RLAgent(agent_type='td', \n",
    "                states=states, \n",
    "                actions=actions, \n",
    "                policy=policy, \n",
    "                policy_type='tabular',\n",
    "                value_type='neural_network', \n",
    "                value_nn_config=value_nn_config,\n",
    "                value_update_type='adam', \n",
    "                beta_m_adam=0.9, \n",
    "                beta_v_adam=0.999, \n",
    "                epsilon_adam=1e-8,\n",
    "                use_value_trace=False, \n",
    "                value_trace_lambda=0)\n",
    "\n",
    "# run experiment\n",
    "rl_experiments = RLExperiments()\n",
    "fig = rl_experiments.td_semigradient(agent, env, \n",
    "                                     state_representation=state_representation.one_hot_encode,\n",
    "                                     num_runs=10,\n",
    "                                     num_episodes=2000,\n",
    "                                     epsilon=1.0,\n",
    "                                     step_size={'value': 0.001},\n",
    "                                     discount=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8908b74f",
   "metadata": {},
   "source": [
    "## Semi-Gradient TD with Neural Network (Adam Update) - PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410c871f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define environment\n",
    "num_states = 50\n",
    "start_state = 25\n",
    "left_terminal_state = 1\n",
    "right_terminal_state= 50\n",
    "max_movement = 10\n",
    "env = EnvironmentRandomWalk(num_states, start_state, left_terminal_state, right_terminal_state, max_movement) \n",
    "\n",
    "# define agent with function approximation\n",
    "state_representation = StateRepresentation(num_states=num_states)\n",
    "\n",
    "actions = ['left', 'right']\n",
    "states = np.zeros((num_states, 1))\n",
    "policy = None\n",
    "\n",
    "value_nn_config = {\n",
    "    'layers': {\n",
    "        'input': {\n",
    "            'nodes': len(states),\n",
    "            'activation_function': 'linear',\n",
    "        },\n",
    "        'hidden_1': {\n",
    "            'nodes': 10,\n",
    "            'activation_function': 'relu',\n",
    "        },\n",
    "        'output': {\n",
    "            'nodes': 1,\n",
    "            'activation_function': 'linear',\n",
    "        },\n",
    "    }\n",
    "}\n",
    "            \n",
    "agent = RLAgent(agent_type='td', \n",
    "                states=states, \n",
    "                actions=actions, \n",
    "                policy=policy, \n",
    "                policy_type='tabular',\n",
    "                value_type='nn_pytorch', \n",
    "                value_nn_config=value_nn_config,\n",
    "                value_update_type='adam', \n",
    "                beta_m_adam=0.9, \n",
    "                beta_v_adam=0.999, \n",
    "                epsilon_adam=1e-8,\n",
    "                use_value_trace=False, \n",
    "                value_trace_lambda=0)\n",
    "\n",
    "# run experiment\n",
    "rl_experiments = RLExperiments()\n",
    "fig = rl_experiments.td_semigradient(agent, env, \n",
    "                                     state_representation=state_representation.one_hot_encode,\n",
    "                                     num_runs=10,\n",
    "                                     num_episodes=2000,\n",
    "                                     epsilon=1.0,\n",
    "                                     step_size={'value': 0.001},\n",
    "                                     discount=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6d0d2d",
   "metadata": {},
   "source": [
    "## Semi-Gradient TD with Neural Network (Adam Update) - Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab20207d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# define environment\n",
    "num_states = 50\n",
    "start_state = 25\n",
    "left_terminal_state = 1\n",
    "right_terminal_state= 50\n",
    "max_movement = 10\n",
    "env = EnvironmentRandomWalk(num_states, start_state, left_terminal_state, right_terminal_state, max_movement) \n",
    "\n",
    "# define agent with function approximation\n",
    "state_representation = StateRepresentation(num_states=num_states)\n",
    "\n",
    "actions = ['left', 'right']\n",
    "states = np.zeros((num_states, 1))\n",
    "policy = None\n",
    "\n",
    "value_nn_config = {\n",
    "    'layers': {\n",
    "        'input': {\n",
    "            'nodes': len(states),\n",
    "            'activation_function': 'linear',\n",
    "        },\n",
    "        'hidden_1': {\n",
    "            'nodes': 10,\n",
    "            'activation_function': 'relu',\n",
    "        },\n",
    "        'output': {\n",
    "            'nodes': 1,\n",
    "            'activation_function': 'linear',\n",
    "        },\n",
    "    }\n",
    "}\n",
    "            \n",
    "agent = RLAgent(agent_type='td', \n",
    "                states=states, \n",
    "                actions=actions, \n",
    "                policy=policy, \n",
    "                policy_type='tabular',\n",
    "                value_type='nn_keras', \n",
    "                value_nn_config=value_nn_config,\n",
    "                value_update_type='adam', \n",
    "                beta_m_adam=0.9, \n",
    "                beta_v_adam=0.999, \n",
    "                epsilon_adam=1e-8,\n",
    "                use_value_trace=False, \n",
    "                value_trace_lambda=0)\n",
    "\n",
    "# run experiment\n",
    "rl_experiments = RLExperiments()\n",
    "fig = rl_experiments.td_semigradient(agent, env, \n",
    "                                     state_representation=state_representation.one_hot_encode,\n",
    "                                     num_runs=1,\n",
    "                                     num_episodes=300,\n",
    "                                     epsilon=1.0,\n",
    "                                     step_size={'value': 0.001},\n",
    "                                     discount=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426c9986",
   "metadata": {},
   "source": [
    "## Semi-Gradient SARSA with Tile Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e247e7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# define environment\n",
    "env = EnvironmentMountainCar() \n",
    "\n",
    "# define agent with function approximation\n",
    "num_tiles = 8\n",
    "num_tilings = 8\n",
    "iht_size = 4096\n",
    "state_representation = StateRepresentation(num_tiles=num_tiles,\n",
    "                                           num_tilings=num_tilings,\n",
    "                                           iht_size=iht_size,\n",
    "                                           min_pose = env.min_position,\n",
    "                                           max_pose = env.max_position,\n",
    "                                           min_vel = -1*env.min_max_velocity,\n",
    "                                           max_vel = env.min_max_velocity,\n",
    "                                          )\n",
    "\n",
    "actions = ['accelerate_left', 'dont_accelerate', 'accelerate_right']\n",
    "states = np.zeros((iht_size, 1))\n",
    "policy = None\n",
    "\n",
    "agent = RLAgent(agent_type='sarsa', \n",
    "                states=states, \n",
    "                actions=actions, \n",
    "                policy=policy, \n",
    "                policy_type='tabular',\n",
    "                value_type='linear',\n",
    "                value_update_type='stochastic_gradient_descent',\n",
    "                use_value_trace=False, \n",
    "                value_trace_lambda=0)\n",
    "\n",
    "# run experiment\n",
    "rl_experiments = RLExperiments()\n",
    "df, fig = rl_experiments.sarsa_semigradient(agent, env, \n",
    "                                            state_representation=state_representation.dynamicstate_tilecoding,\n",
    "                                            num_runs=3,\n",
    "                                            num_episodes=100,\n",
    "                                            epsilon=0.1,\n",
    "                                            step_size={'value': 0.1 / num_tilings},\n",
    "                                            discount=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844259af",
   "metadata": {},
   "source": [
    "## TD Actor-Critic with Eligibility Traces (Linear Policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f86e620",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# define environment\n",
    "env = EnvironmentPendulum() \n",
    "\n",
    "# define agent with function approximation\n",
    "num_tiles = 8\n",
    "num_tilings = 32\n",
    "iht_size = 4096\n",
    "state_representation = StateRepresentation(num_tiles=num_tiles,\n",
    "                                           num_tilings=num_tilings,\n",
    "                                           iht_size=iht_size,\n",
    "                                           min_pose = -1*np.pi,\n",
    "                                           max_pose = np.pi,\n",
    "                                           min_vel = -1*env.min_max_velocity,\n",
    "                                           max_vel = env.min_max_velocity,\n",
    "                                          )\n",
    "\n",
    "actions = ['accelerate_left', 'dont_accelerate', 'accelerate_right']\n",
    "states = np.zeros((iht_size, 1))\n",
    "policy = None\n",
    "\n",
    "agent = RLAgent(agent_type='td', \n",
    "                states=states, \n",
    "                actions=actions, \n",
    "                policy=policy, \n",
    "                use_average_reward=True,\n",
    "                policy_type='linear',\n",
    "                policy_softmax_tau = 1.0,\n",
    "                policy_update_type='stochastic_gradient_descent',\n",
    "                value_type='linear',\n",
    "                value_update_type='stochastic_gradient_descent',\n",
    "                use_value_trace=True, \n",
    "                value_trace_lambda=0.75,\n",
    "                use_policy_trace=True, \n",
    "                policy_trace_lambda=0.75)\n",
    "\n",
    "# run experiment\n",
    "rl_experiments = RLExperiments()\n",
    "\n",
    "step_sizes = {\n",
    "    'value': 2e-2 / num_tilings,\n",
    "    'policy': 2e-2 / num_tilings,\n",
    "    'avg_reward': 2e-6,\n",
    "}\n",
    "\n",
    "df, fig = rl_experiments.actor_critic(agent, env, \n",
    "                                      state_representation=state_representation.dynamicstate_tilecoding,\n",
    "                                      num_runs=10,\n",
    "                                      max_steps=20000,\n",
    "                                      step_size=step_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486b2d1a",
   "metadata": {},
   "source": [
    "## TD Actor-Critic with Eligibility Traces (Neural Network Policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a642b84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define environment\n",
    "env = EnvironmentPendulum() \n",
    "\n",
    "# define agent with function approximation\n",
    "num_tiles = 8\n",
    "num_tilings = 32\n",
    "iht_size = 4096\n",
    "state_representation = StateRepresentation(num_tiles=num_tiles,\n",
    "                                           num_tilings=num_tilings,\n",
    "                                           iht_size=iht_size,\n",
    "                                           min_pose = -1*np.pi,\n",
    "                                           max_pose = np.pi,\n",
    "                                           min_vel = -1*env.min_max_velocity,\n",
    "                                           max_vel = env.min_max_velocity,\n",
    "                                          )\n",
    "\n",
    "actions = ['accelerate_left', 'dont_accelerate', 'accelerate_right']\n",
    "states = np.zeros((iht_size, 1))\n",
    "policy = None\n",
    "\n",
    "policy_nn_config = {\n",
    "    'layers': {\n",
    "        'input': {\n",
    "            'nodes': len(states),\n",
    "            'activation_function': 'linear',\n",
    "        },\n",
    "        'hidden_1': {\n",
    "            'nodes': 100,\n",
    "            'activation_function': 'relu',\n",
    "        },\n",
    "        'output': {\n",
    "            'nodes': len(actions),\n",
    "            'activation_function': 'linear',\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "agent = RLAgent(agent_type='td', \n",
    "                states=states, \n",
    "                actions=actions, \n",
    "                policy=policy, \n",
    "                use_average_reward=True,\n",
    "                policy_type='neural_network',\n",
    "                policy_nn_config=policy_nn_config,\n",
    "                policy_softmax_tau = 1.0,\n",
    "                policy_update_type='adam',\n",
    "                value_type='linear',\n",
    "                value_update_type='stochastic_gradient_descent',\n",
    "                use_value_trace=True, \n",
    "                value_trace_lambda=0.75,\n",
    "                use_policy_trace=True, \n",
    "                policy_trace_lambda=0.75)\n",
    "\n",
    "# run experiment\n",
    "rl_experiments = RLExperiments()\n",
    "\n",
    "step_sizes = {\n",
    "    'value': 2e-2 / num_tilings,\n",
    "    'policy': 2e-3 / num_tilings,\n",
    "    'avg_reward': 2e-6,\n",
    "}\n",
    "\n",
    "df, fig = rl_experiments.actor_critic(agent, env, \n",
    "                                      state_representation=state_representation.dynamicstate_tilecoding,\n",
    "                                      num_runs=1,\n",
    "                                      max_steps=20000,\n",
    "                                      step_size=step_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101039f4",
   "metadata": {},
   "source": [
    "## TD Actor-Critic (Neural Network Policy - PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7538c01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define environment\n",
    "env = EnvironmentPendulum() \n",
    "\n",
    "# define agent with function approximation\n",
    "num_tiles = 8\n",
    "num_tilings = 32\n",
    "iht_size = 4096\n",
    "state_representation = StateRepresentation(num_tiles=num_tiles,\n",
    "                                           num_tilings=num_tilings,\n",
    "                                           iht_size=iht_size,\n",
    "                                           min_pose = -1*np.pi,\n",
    "                                           max_pose = np.pi,\n",
    "                                           min_vel = -1*env.min_max_velocity,\n",
    "                                           max_vel = env.min_max_velocity,\n",
    "                                          )\n",
    "\n",
    "actions = ['accelerate_left', 'dont_accelerate', 'accelerate_right']\n",
    "states = np.zeros((iht_size, 1))\n",
    "policy = None\n",
    "\n",
    "policy_nn_config = {\n",
    "    'layers': {\n",
    "        'input': {\n",
    "            'nodes': len(states),\n",
    "            'activation_function': 'linear',\n",
    "        },\n",
    "        'hidden_1': {\n",
    "            'nodes': 100,\n",
    "            'activation_function': 'relu',\n",
    "        },\n",
    "        'output': {\n",
    "            'nodes': len(actions),\n",
    "            'activation_function': 'linear',\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "agent = RLAgent(agent_type='td', \n",
    "                states=states, \n",
    "                actions=actions, \n",
    "                policy=policy, \n",
    "                use_average_reward=True,\n",
    "                policy_type='nn_pytorch',\n",
    "                policy_nn_config=policy_nn_config,\n",
    "                policy_softmax_tau = 1.0,\n",
    "                policy_update_type='stochastic_gradient_descent',\n",
    "                value_type='linear',\n",
    "                value_update_type='stochastic_gradient_descent',\n",
    "                use_value_trace=False, \n",
    "                value_trace_lambda=0,\n",
    "                use_policy_trace=False, \n",
    "                policy_trace_lambda=0)\n",
    "\n",
    "# run experiment\n",
    "rl_experiments = RLExperiments()\n",
    "\n",
    "step_sizes = {\n",
    "    'value': 2e-2 / num_tilings,\n",
    "    'policy': 2e-3 / num_tilings,\n",
    "    'avg_reward': 2e-6,\n",
    "}\n",
    "\n",
    "df, fig = rl_experiments.actor_critic(agent, env, \n",
    "                                      state_representation=state_representation.dynamicstate_tilecoding,\n",
    "                                      num_runs=1,\n",
    "                                      max_steps=50000,\n",
    "                                      step_size=step_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ff5f12",
   "metadata": {},
   "source": [
    "## DQN with Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b88b08",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# define environment\n",
    "env = EnvironmentLunarLanding() \n",
    "\n",
    "# define agent function approximation\n",
    "actions = ['main_thruster', 'left_thruster', 'right_thruster', 'no_thruster']\n",
    "\n",
    "# state: (velocity_x, velocity_y, angle, position_x, position_y, landing_zone_x, landing_zone_y, fuel)\n",
    "states = np.zeros((8, 1))\n",
    "\n",
    "policy = None\n",
    "\n",
    "value_nn_config = {\n",
    "    'layers': {\n",
    "        'input': {\n",
    "            'nodes': len(states),\n",
    "            'activation_function': 'linear',\n",
    "        },\n",
    "        'hidden_1': {\n",
    "            'nodes': 256,\n",
    "            'activation_function': 'relu',\n",
    "        },\n",
    "        'output': {\n",
    "            'nodes': len(actions),\n",
    "            'activation_function': 'linear',\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "use_experience_replay = True\n",
    "\n",
    "agent = RLAgent(agent_type='expected_sarsa', \n",
    "                states=states, \n",
    "                actions=actions, \n",
    "                policy=policy, \n",
    "                policy_type='softmax', \n",
    "                policy_softmax_tau = 0.001, \n",
    "                value_type='neural_network', \n",
    "                value_nn_config=value_nn_config,\n",
    "                value_update_type='adam', \n",
    "                beta_m_adam=0.9, \n",
    "                beta_v_adam=0.999, \n",
    "                epsilon_adam=1e-8,\n",
    "                use_experience_replay=use_experience_replay, \n",
    "                replay_buffer_size=50000, \n",
    "                replay_buffer_minibatch_size=8,\n",
    "                use_value_trace=False, \n",
    "                value_trace_lambda=0)\n",
    "\n",
    "# run experiment\n",
    "rl_experiments = RLExperiments()\n",
    "df, fig = rl_experiments.dqn_replay_buffer(agent, env,\n",
    "                                           num_runs=3,\n",
    "                                           num_episodes=300,\n",
    "                                           step_size={'value': 1e-3},\n",
    "                                           discount=0.99,\n",
    "                                           experience_replay=use_experience_replay,\n",
    "                                           experience_replay_steps=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99471f45",
   "metadata": {},
   "source": [
    "## DQN with Experience Replay - PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d453539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define environment\n",
    "env = EnvironmentLunarLanding() \n",
    "\n",
    "# define agent function approximation\n",
    "actions = ['main_thruster', 'left_thruster', 'right_thruster', 'no_thruster']\n",
    "\n",
    "# state: (velocity_x, velocity_y, angle, position_x, position_y, landing_zone_x, landing_zone_y, fuel)\n",
    "states = np.zeros((8, 1))\n",
    "\n",
    "policy = None\n",
    "\n",
    "value_nn_config = {\n",
    "    'layers': {\n",
    "        'input': {\n",
    "            'nodes': len(states),\n",
    "            'activation_function': 'linear',\n",
    "        },\n",
    "        'hidden_1': {\n",
    "            'nodes': 256,\n",
    "            'activation_function': 'relu',\n",
    "        },\n",
    "        'output': {\n",
    "            'nodes': len(actions),\n",
    "            'activation_function': 'linear',\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "use_experience_replay = True\n",
    "\n",
    "agent = RLAgent(agent_type='expected_sarsa', \n",
    "                states=states, \n",
    "                actions=actions, \n",
    "                policy=policy, \n",
    "                policy_type='softmax', \n",
    "                policy_softmax_tau = 0.001, \n",
    "                value_type='nn_pytorch', \n",
    "                value_nn_config=value_nn_config,\n",
    "                value_update_type='adam', \n",
    "                beta_m_adam=0.9, \n",
    "                beta_v_adam=0.999, \n",
    "                epsilon_adam=1e-8,\n",
    "                use_experience_replay=use_experience_replay, \n",
    "                replay_buffer_size=50000, \n",
    "                replay_buffer_minibatch_size=8,\n",
    "                use_value_trace=False, \n",
    "                value_trace_lambda=0)\n",
    "\n",
    "# run experiment\n",
    "rl_experiments = RLExperiments()\n",
    "df, fig = rl_experiments.dqn_replay_buffer(agent, env,\n",
    "                                           num_runs=3,\n",
    "                                           num_episodes=300,\n",
    "                                           step_size={'value': 1e-3},\n",
    "                                           discount=0.99,\n",
    "                                           experience_replay=use_experience_replay,\n",
    "                                           experience_replay_steps=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642b2527",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
